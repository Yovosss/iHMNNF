{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import argparse\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.backend import clear_session\n",
    "from keras import regularizers \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout, Activation, BatchNormalization, InputLayer, Embedding, Reshape, Concatenate, Masking, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "from optuna.trial import TrialState\n",
    "from optuna.samplers import RandomSampler\n",
    "from optuna.visualization import plot_intermediate_values\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_slice\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc, classification_report, precision_recall_curve, average_precision_score\n",
    "\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "\n",
    "from Data_helper import LoadDataTrial\n",
    "from utils.layers import ExternalMasking\n",
    "from utils.grud_layers import GRUD, Bidirectional_for_GRUD\n",
    "from BuildModel import LoadModel\n",
    "from utils.attention_function import attention_3d_block_spatial as PreAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: Namespace(batch_size=32, dataset_name='phase_viii', early_stopping_patience=10, epochs=100, fold=0, label=3, max_timestamp=172800, max_timestep=200, model_type='GRUD', trainORvalidation='Validation', use_bidirectional_rnn=False, working_path='.')\n"
     ]
    }
   ],
   "source": [
    "arg_parser = argparse.ArgumentParser()\n",
    "arg_parser.add_argument('--working_path', default='.')\n",
    "\n",
    "## data\n",
    "arg_parser.add_argument('--dataset_name', default='phase_viii', \n",
    "                         help='The data files should be saved in [working_path]/data/[dataset_name] directory.')\n",
    "arg_parser.add_argument('--fold', type=int, default=0, \n",
    "                         help='the fold data taken to use, there are 5 folds or 10 folds')\n",
    "arg_parser.add_argument('--label', default=3, type=int, choices=[-1,0,1,2,3,4],\n",
    "                         help='the label type')\n",
    "\n",
    "## model\n",
    "arg_parser.add_argument('--model_type', default='GRUD', choices=['LR', 'RF', 'SVM', 'DNN', 'DNN_EE', 'GRUD', 'HMM_EF', 'HMM_LF'])\n",
    "arg_parser.add_argument('--max_timestep', type=int, default=200, \n",
    "                        help='Time series of at most # time steps are used. Default: 200.')\n",
    "arg_parser.add_argument('--max_timestamp', type=int, default=48*60*60,choices=[120*60*60, 72*60*60, 96*60*60, 120*60*60],\n",
    "                        help='Time series of at most # seconds are used. Default: 48 (hours).')\n",
    "arg_parser.add_argument('--use_bidirectional_rnn', default=False)\n",
    "# Train\n",
    "arg_parser.add_argument('--trainORvalidation', default='Validation', choices=['Train', 'Validation'])\n",
    "arg_parser.add_argument('--epochs', type=int, default=100)\n",
    "arg_parser.add_argument('--early_stopping_patience', type=int, default=10)\n",
    "arg_parser.add_argument('--batch_size', type=int, default=32)\n",
    "# set the actual arguments if running in notebook\n",
    "if not (__name__ == '__main__' and '__file__' in globals()):\n",
    "    ARGS = arg_parser.parse_args([\n",
    "        '--model_type', 'GRUD',\n",
    "        '--dataset_name', 'phase_viii',\n",
    "        '--fold', '0',\n",
    "        '--epochs', '100',\n",
    "        '--trainORvalidation', 'Validation'\n",
    "    ])\n",
    "else:\n",
    "      ARGS = arg_parser.parse_args()\n",
    "\n",
    "print('Arguments:', ARGS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cm(label_info, y_validation, y_pred, runtime):\n",
    "    # get the confusion matrix\n",
    "    C = confusion_matrix(y_validation, y_pred)\n",
    "    if label_info == 'flatten multi-classification':\n",
    "        df_cm = pd.DataFrame(C, columns=['0','1','2','3','4','5','6'])\n",
    "    elif label_info == 'bacterial, viral and others':\n",
    "        df_cm = pd.DataFrame(C, columns=['0','1','2'])\n",
    "    else:\n",
    "        df_cm = pd.DataFrame(C, columns=['0','1'])\n",
    "    df_cm['bootstrap_index'] = runtime\n",
    "    df_cm['label'] = label_info\n",
    "\n",
    "    return df_cm\n",
    "    \n",
    "def save_report(label_info, y_validation, y_pred, runtime):\n",
    "    report = classification_report(y_validation, y_pred,digits=5, output_dict = True)\n",
    "    report = pd.DataFrame(report).transpose().reset_index()\n",
    "    report['bootstrap_index'] = runtime\n",
    "    report['label'] = label_info\n",
    "\n",
    "    return report\n",
    "\n",
    "def plot_roc(nclasses, y_validation, probas):\n",
    "    '''\n",
    "    label_info: The label information to tag the figure name\n",
    "    nclasses: The dim of y, which determines the way of plot\n",
    "    y_validation: The y value of validation dataset\n",
    "    probas: The probability of model results\n",
    "    '''\n",
    "    # Compute ROC curve and area the curve\n",
    "    if nclasses == 2:\n",
    "        fpr, tpr, thresholds = roc_curve(y_validation, probas[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        roc_value = {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}\n",
    "    else:\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for m in range(nclasses):\n",
    "            fpr[m], tpr[m], _ = roc_curve(y_validation[:, m], probas[:, m])\n",
    "            roc_auc[m] = auc(fpr[m], tpr[m])\n",
    "        # Compute macro-average ROC curve and ROC area\n",
    "        all_fpr = np.unique(np.concatenate([fpr[n] for n in range(nclasses)]))\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for k in range(nclasses):\n",
    "            mean_tpr += interp(all_fpr, fpr[k], tpr[k])\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= nclasses\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "        # save the data\n",
    "        roc_value = {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}\n",
    "\n",
    "    return roc_value\n",
    "\n",
    "def plot_prc(nclasses, y_validation, y_probas, y_pred):\n",
    "    '''\n",
    "    label_info: The label information to tag the figure name\n",
    "    ydim: The dim of y, which determines the way of plot\n",
    "    y_validation: The y value of validation dataset\n",
    "    probas: The probability of model results\n",
    "    '''\n",
    "    # Compute PRC curve and area the curve\n",
    "    if nclasses == 2:\n",
    "        precision, recall, _ = precision_recall_curve(y_validation, y_probas[:, 1])\n",
    "        f1, auprc = f1_score(y_validation, y_pred), auc(recall, precision)\n",
    "        \n",
    "        # save the data\n",
    "        prc_value = {'precision': precision, 'recall': recall, 'auprc': auprc}\n",
    "\n",
    "    else:\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        average_precision = dict()\n",
    "        for i in range(nclasses):\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_validation[:, i], y_probas[:, i])\n",
    "            average_precision[i] = average_precision_score(y_validation[:, i], y_probas[:, i])\n",
    "        \n",
    "        precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_validation.ravel(), y_probas.ravel())\n",
    "        average_precision[\"micro\"] = average_precision_score(y_validation, y_probas, average=\"micro\")\n",
    "\n",
    "        # save the data\n",
    "        prc_value = {'precision': precision, 'recall': recall, 'auprc': average_precision}\n",
    "    \n",
    "    return prc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zju/anaconda3/envs/gru/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning:\n",
      "\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info]: The GRU-D model test of AD and AID is processing\n",
      "[Info]: The Best model of (AD and AID) is GRUD(AD and AID)_trial#60.h5\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# define the label, -1 represent the multi-class classification\n",
    "model_path = os.path.join(ARGS.working_path, 'model_tuning', 'phase_viii', '48hours', '20230610', 'GRUD_MtoAtten_7dim_48hrs_Paper(optuna)')\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "output_path = os.path.join(ARGS.working_path, 'test', 'phase_viii', '48hours', '20230610', 'GRUD_MtoAtten_7dim_48hrs_Paper(optuna)')\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "T = time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n",
    "LABEL_DICT = {'-1':'flatten multi-classification', \n",
    "              '0': 'infectious and non-infectious', \n",
    "              '1':'bacterial, viral and others', \n",
    "              '2':'NIID and tumor', \n",
    "              '3': 'AD and AID',\n",
    "              '4':'HM and SM'}\n",
    "label_type = [4, 3, 2, 1, 0]\n",
    "all_avg = {}\n",
    "for label in label_type:\n",
    "    \n",
    "    # Load the data\n",
    "    dataset = LoadDataTrial(\n",
    "        data_path=os.path.join(ARGS.working_path, 'data', ARGS.dataset_name, '48hours', 'processed', 'raw/data4hc_v20220401'), \n",
    "        model_type=ARGS.model_type,\n",
    "        label_name=label,\n",
    "        max_timestep=ARGS.max_timestep,\n",
    "        max_timestamp=ARGS.max_timestamp)\n",
    "\n",
    "    # Load the data\n",
    "    X_train, y_train, nclasses_train, folds_train, shapes_train = dataset.training_generator(ARGS.fold)\n",
    "    X_validation, y_validation, nclasses_validation,folds_validation, shapes_validation = dataset.validation_generator(ARGS.fold)\n",
    "    X_test, y_test, nclasses_test, folds_test, shapes_test = dataset.test_generator(ARGS.fold)\n",
    "\n",
    "    # hyperparameter tuning\n",
    "    print(\"[Info]: The GRU-D model test of {0} is processing\".format(LABEL_DICT[str(label)]))\n",
    "    #take the best model\n",
    "    dir_model = []\n",
    "    idx_model = np.array([])\n",
    "    for i in os.listdir(model_path):\n",
    "        if LABEL_DICT[str(label)] in i and '.h5' in i:\n",
    "            dir_model.append(i)\n",
    "    if len(dir_model) == 0:\n",
    "        continue\n",
    "    for j in dir_model:\n",
    "        seachobj = re.search(r\"\\d+(?=\\.h5)\", j)\n",
    "        idx_model = np.append(idx_model, int(seachobj.group()))\n",
    "    target_model = dir_model[np.argmax(idx_model)]\n",
    "    # load the model\n",
    "    print(\"[Info]: The Best model of ({0}) is {1}\".format(LABEL_DICT[str(label)], target_model))\n",
    "    model = LoadModel(os.path.join(model_path, target_model))\n",
    "\n",
    "    # set bootstrap on test dataset\n",
    "    if label == -1:\n",
    "        df_cms = pd.DataFrame(columns=['bootstrap_index', 'label', '0','1','2','3','4','5','6'])\n",
    "    elif label == 1:\n",
    "        df_cms = pd.DataFrame(columns=['bootstrap_index', 'label', '0','1','2'])\n",
    "    else:\n",
    "        df_cms = pd.DataFrame(columns=['bootstrap_index', 'label', '0','1'])\n",
    "    df_reports = pd.DataFrame(columns=['bootstrap_index', 'label', 'index', 'precision', 'recall', 'f1-score', 'support'])\n",
    "    npy_roc = {}\n",
    "    npy_prc = {}\n",
    "    for runtime in tqdm(range(1000)):\n",
    "        # print(\"[Info]: The index of bootstrap is {}\".format(runtime))\n",
    "        idx = np.random.randint(0, len(X_test[0]) - 1, size=len(X_test[0]))\n",
    "        X_test_boot = [x[idx] for x in X_test]\n",
    "        y_test_boot = y_test[idx]\n",
    "\n",
    "        # predict the test dataset\n",
    "        y_pred_proba = model.predict(X_test_boot)\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "        # plot the confusion matrix\n",
    "        df_cm = get_cm(LABEL_DICT[str(label)], y_test_boot, y_pred, runtime)\n",
    "        df_cms = pd.concat([df_cms, df_cm], axis=0, ignore_index=True)\n",
    "\n",
    "        # save the test dataset results\n",
    "        df_report = save_report(LABEL_DICT[str(label)], y_test_boot, y_pred, runtime)\n",
    "        df_reports = pd.concat([df_reports, df_report], axis=0, ignore_index=True)\n",
    "\n",
    "        # # plot the ROC and PRC curve\n",
    "        if label == -1:\n",
    "            y_test_one_hot = label_binarize(y_test_boot, classes=[0,1,2,3,4,5,6])\n",
    "            roc_dict = plot_roc(nclasses_train, y_test_one_hot, y_pred_proba)\n",
    "            prc_dict = plot_prc(nclasses_train, y_test_one_hot, y_pred_proba, y_pred)\n",
    "        elif label == 1:\n",
    "            y_test_one_hot = label_binarize(y_test_boot, classes=[0,1,2])\n",
    "            roc_dict = plot_roc(nclasses_train, y_test_one_hot, y_pred_proba)\n",
    "            prc_dict = plot_prc(nclasses_train, y_test_one_hot, y_pred_proba, y_pred)\n",
    "        else:\n",
    "            roc_dict = plot_roc(nclasses_train, y_test_boot, y_pred_proba)\n",
    "            prc_dict = plot_prc(nclasses_train, y_test_boot, y_pred_proba, y_pred)\n",
    "\n",
    "        npy_roc[runtime] = roc_dict\n",
    "        npy_prc[runtime] = prc_dict\n",
    "\n",
    "    df_cms.to_csv(os.path.join(output_path, 'confusion_matrix(test_bootstrap)({}).csv'.format(LABEL_DICT[str(label)])))\n",
    "    df_reports.to_csv(os.path.join(output_path, 'classification_reuslts(test_bootstrap)({}).csv'.format(LABEL_DICT[str(label)])))\n",
    "    np.save(os.path.join(output_path, 'roc(test_bootstrap)({}).npy'.format(LABEL_DICT[str(label)])), npy_roc)\n",
    "    np.save(os.path.join(output_path, 'prc(test_bootstrap)({}).npy'.format(LABEL_DICT[str(label)])), npy_prc)\n",
    "\n",
    "    # calculate the average value\n",
    "    roc_list = []\n",
    "    prc_list = []\n",
    "    if label in [0,2,3,4]:\n",
    "        for key, value in npy_roc.items():\n",
    "            roc_list.append(value['roc_auc'])\n",
    "\n",
    "        for key, value in npy_prc.items():\n",
    "            prc_list.append(value['auprc'])\n",
    "    else:\n",
    "        for key, value in npy_roc.items():\n",
    "            roc_list.append(value['roc_auc']['macro'])\n",
    "\n",
    "        for key, value in npy_prc.items():\n",
    "            prc_list.append(value['auprc']['micro'])\n",
    "    \n",
    "    roc_mean = np.mean(roc_list)\n",
    "    roc_std = np.std(roc_list)\n",
    "    prc_mean = np.mean(prc_list)\n",
    "    prc_std = np.std(prc_list)\n",
    "\n",
    "    all_avg[LABEL_DICT[str(label)]] = {'roc_mean': roc_mean,\n",
    "                                       'roc_std': roc_std,\n",
    "                                       'prc_mean': prc_mean,\n",
    "                                       'prc_std': prc_std}\n",
    "np.save(os.path.join(output_path, 'statistics_test(GRUD).npy'), all_avg)\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AD and AID': {'roc_mean': 0.5552544369818777,\n",
       "  'roc_std': 0.041451148332846605,\n",
       "  'prc_mean': 0.3588655209332339,\n",
       "  'prc_std': 0.04868575891599155}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.load('/mnt/data/wzx/jupyter_notebook/HC4FUO/test/phase_viii/48hours/20230610/GRUD_MtoAtten_7dim_48hrs_Paper(optuna)/statistics_test(GRUD).npy', allow_pickle=True).tolist()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'infectious and non-infectious': {'roc_mean': 0.6683505841209988,\n",
       "  'roc_std': 0.013860344425626746,\n",
       "  'prc_mean': 0.4427750236980253,\n",
       "  'prc_std': 0.02331864673810696},\n",
       " 'bacterial, viral and others': {'roc_mean': 0.5826408601549004,\n",
       "  'roc_std': 0.014169492245044293,\n",
       "  'prc_mean': 0.7009753197492378,\n",
       "  'prc_std': 0.014118001947704427},\n",
       " 'NIID and tumor': {'roc_mean': 0.6634060425307373,\n",
       "  'roc_std': 0.023127103512989366,\n",
       "  'prc_mean': 0.7209464224386555,\n",
       "  'prc_std': 0.02940540670980321},\n",
       " 'AD and AID': {'roc_mean': 0.5735573809820168,\n",
       "  'roc_std': 0.04178544806578962,\n",
       "  'prc_mean': 0.36862980407635554,\n",
       "  'prc_std': 0.05131382007699208},\n",
       " 'HM and SM': {'roc_mean': 0.7926923095794027,\n",
       "  'roc_std': 0.02419556654005299,\n",
       "  'prc_mean': 0.789870920982252,\n",
       "  'prc_std': 0.03439994764456762}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.load('/mnt/data/wzx/jupyter_notebook/HC4FUO/test/phase_viii/120hours/20220604/GRUD_PreAttenSpatial_7dim_120hrs_Paper(optuna)/statistics_test(GRUD).npy', allow_pickle=True).tolist()\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9778b63409256c7e2a3f4339e1b2720e0533322a81e29f0ed5374e7c0c5df233"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('gru')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "4e1aa0fb0bd10bf5277393d95b1f7e9c2d03411dc295f291055dba03e839c981"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
